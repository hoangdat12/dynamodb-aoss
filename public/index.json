[
{
	"uri": "//localhost:1313/",
	"title": "Replicate and synchronize data from DynamoDB to OpenSearch Serverless",
	"tags": [],
	"description": "",
	"content": "Replicate and synchronize data from DynamoDB to OpenSearch Serverless Overview In this lab, you will learn about the concepts of DynamoDB, OpenSearch Serverless and how we can replicate and synchronize data from DynamoDB to OpenSearch Serverless to optimize search operation, Praticing DynamoDB, OpenSearch Serverless\nContent Introduction Prerequisite Replicate Data Synchronize Data Clean up resource Implement with Terraform "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createdynamodb/",
	"title": "Create DynamoDb table",
	"tags": [],
	"description": "",
	"content": "DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. DynamoDB lets you offload the administrative burdens of operating and scaling a distributed database so that you don\u0026rsquo;t have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling.\nIn this step, we need to create a DynamoDB table\nCreate DynamoDB table Go to the AWS DynamoDB Console Select Tables Click Create table When creating a Table and defining your DynamoDB schema, one of the first options you’ll be asked is to specify your Table’s Partition Key and Sort Key. This is an important decision that has impact on how your table’s record’s can be accessed.\nA DynamoDB Partition Key serves as the primary identifier for partitioning your data across DynamoDB\u0026rsquo;s multiple storage nodes. It\u0026rsquo;s a mandatory component when setting up a DynamoDB table. The Partition Key helps distribute your data across different partitions\nA DynamoDB Sorted Key is an optional attribute that allows for the sorting of items within each partition. By specifying a Sort Key, you enable DynamoDB to order the records within a partition based on the Sort Key\u0026rsquo;s value\nIn Create table step At Partition key, enter id At Table settings, select Default settings Click Create Check create table success After clicking Create, wait a few moments, and you will see the lab-table appear. 3.Create an IAM Role for API Gateway can access DynamoDB\nWe need to create IAM Role for API Gateway have permission put Item into DynamoDB To create IAM Role\nGo to the AWS IAM Console Select Policies Click Create policy In Create policy step Click Json Coppy and paste into Policy editor { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Statement1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34; ], \u0026#34;Resource\u0026#34;: [\u0026#34;arn:aws:dynamodb:\u0026lt;your-region\u0026gt;:\u0026lt;your-aws-account-id\u0026gt;:table/\u0026lt;you-dynamodb-table\u0026gt;\u0026#34;] } ] } Click Next In Review step Enter dynamodb-putitem-policy Scroll down and click Create policy After create policy, we will create a IAM Role for API Gateway\nIn AWS IAM Console Select Roles Click Create role In Select trusted entity step Select AWS service At Service or use case, select API Gateway At Use case, select API Gateway In Add permissions step Click Next In Name, review, and create step Enter dynamodb-api-gw-role Scroll down and click Create role After create dynamodb-api-gw-role, we need to add dynamodb-putitem-policy to this role\nSelect Roles Search dynamodb-api-gw-role Click dynamodb-api-gw-role Click Add permissions Click Attach policies Search and select dynamodb-api-gw-role Click Add permissions "
},
{
	"uri": "//localhost:1313/3-replicate/3.1-exportdynamodbdatatos3/",
	"title": "Export DynamoDB Data",
	"tags": [],
	"description": "",
	"content": "To replicate data from DynamoDB to Amazon OpenSearch Serverless, a preliminary step involves exporting the data to an Amazon S3 bucket.\nExport DynamoDB data Go to the AWS S3 Console Select Exports to S3 Click Export to S3 To export data from DynamoDB to S3, we need enabled PITR (Point-in-time recovery). Point-in-time recovery helps protect your DynamoDB tables from accidental write or delete operations. With point-in-time recovery, you don\u0026rsquo;t have to worry about creating, maintaining, or scheduling on-demand backups.\nIn Export to S3 step At Source table, choose lab-table Click Turn on PITR Enter S3 destination s3://dynamodb-export-data-lab Select This AWS account Select Full export Select Current time Select DynamoDB JSON Click Export After exporting success, we can click Destination S3 bucket to check.\nWe will see file export in AWSDynamoDB/ folder.\n"
},
{
	"uri": "//localhost:1313/6.terraform/6.1createresource/",
	"title": "Initialize resources with Terraform",
	"tags": [],
	"description": "",
	"content": "Prepare code Click here to download code Open code and go to the terraform file Environment settings Install Terraform Execute the command to install Terraform sudo yum install -y yum-utils shadow-utils sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo sudo yum -y install terraform Configure local to interact with AWS, execute command aws configure Initialize Resource Execute the command terraform init to initialize resources for Terraform Execute the command terraform validate to check the syntax of Terraform configuration files Execute the command terraform plan to view created resoures or change of the terraform infrastructure Execute the command terraform apply -auto-approve to apply to aws Conduct testing in AWS Console Go to EC2 and check to see if there are 3 Instances, jenkins instance and 2 nodes Go to the EKS and check to eks-cluster Preparation Create Github Access Token Go to the Github Setting -\u0026gt; Scroll down and select Developing Setting -\u0026gt; Select Personal access token and click Token (classic) Select Generate new token and click Generate new token (classic) Enter jenkins-access Check all the buttons below Click Generate token To better understand Github Access Token configuration, we can refer to here\nCoppy Token was created Go to the Jenkins Browser At Dashboard select Manages Jenkins Select Credentials Click globals Click Add credentials Select Kind is Username with password Enter Github username and token was created Enter ID jenkins-git Select Create Create Docker hub Access Token Go to the Docker hub -\u0026gt; Select My Account Select Security Select New Access Token Enter Access token Description Click Generate Copy Docker Access Token and do the same to create Github Access Token in Jenkins Go to the Jenkins Browser At Dashboard select Manages Jenkins Select Credentials Click globals Click Add credentials Select Kind is Username with password Nhập Docker username and token was created Enter ID docker-crd Select Create Create AWS_ACCESS_KEY_ID Go to the Jenkins Browser At page Dashboard select Manages Jenkins Select Credentials Click globals Click Add credentials Select Kind is Secret text Enter Docker secret of aws_access_key_id in file.cvs Enter ID AWS_ACCESS_KEY_ID Select Create Tạo AWS_SECRET_ACCESS_KEY Go to the Jenkins Browser At page Dashboard select Manages Jenkins Select Credentials Click globals Click Add credentials Select Kind is Secret text Enter Docker secret of aws_secret_access_key in file.cvs Enter ID AWS_SECRET_ACCESS_KEY Chọn Create Complete After finishing, Our Jenkins Credentails will have 4 files as follows CICD Implement CICD as Chapter 4\n"
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Serverless is a cloud-native development model that allows developers to build and run applications without having to manage servers.\nDynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. DynamoDB lets you offload the administrative burdens of operating and scaling a distributed database so that you don\u0026rsquo;t have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling.\nOpenSearch Serverless Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment.\nKinesis Stream is a serverless streaming data service that simplifies the capture, processing, and storage of data streams at any scale, it designed to process large-scale data streams from a multitude of services in real-time.\nLambda is a serverless compute service, executes your code in response to events, handling compute resources for you.\nIn this lab, you will gain hands-on experience with integrating Amazon DynamoDB and Amazon OpenSearch Serverless. They will learn how to effectively replicate and synchronize data across these services using stream-based mechanisms like DynamoDB Streams and Kinesis Streams.\n"
},
{
	"uri": "//localhost:1313/4-synchronize/4.1-dynamodbstream/",
	"title": "Synchronize Data With DynamoDB Stream",
	"tags": [],
	"description": "",
	"content": "A DynamoDB Stream is a time-ordered sequence of events recording all the modifications for DynamoDB tables in near real-time. Similar to change data capture, DynamoDB Streams consist of multiple Insert, Update, and Delete events. Each record has a unique sequence number which is used for ordering.\nThe architecture overview after you complete this step will be as follows:\nCreate Lambda function Go to the AWS Lambda Console Select Functions Click Create function In Create function step Select Author from scratch At Runtime, select Python 3.12 (newest version) Enter sync-data-with-dynamodb-stream At Change default execution role, choose Use an existing role Select lambda-lab-role lambda-lab-role is the IAM Role that we are created in the previous step\nClick Create function Download Lambda function\nClick Upload and choose the file that you downloaded, then proceed to upload it\nIn DynamoDB stream, event will have three types are: INSERT, REMOVE and MODIFY\nTurn on Dynamodb Stream To synchronize data with DynamoDB Stream, we need enable DynamoDB Stream in DynamoDB table\nGo to the AWS DynamoDB Console Select Tables Click lab-table In lab-table Select Exports and Streams At DynamoDB stream details, click Turn on In Turn on DynamoDB stream Select New image Click Turn on stream Now, after Turn on stream, we need to create a lambda function to receive record that triggered from DynamoDB stream, but before do that, we need edit lambda-lap-role to allow lambda create received record from DynamoDB stream.\nGo to the AWS IAM Console Select Roles Search and select 006-editlambdarole.png In lambda-lap-role Click Add permissions Click Create inline policy In Specify permissions step Click Json Coppy and paste { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;APIAccessForDynamoDBStreams\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:GetRecords\u0026#34;, \u0026#34;dynamodb:GetShardIterator\u0026#34;, \u0026#34;dynamodb:DescribeStream\u0026#34;, \u0026#34;dynamodb:ListStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:\u0026lt;aws-region\u0026gt;:\u0026lt;aws-account-id\u0026gt;:table/\u0026lt;dynamodb-table-name\u0026gt;/stream/*\u0026#34; } ] } Scroll down and lick Next In Review and create Enter dynamodb-stream-data Click Create policy After create dynamodb-stream-data, we need create aoss-policy with policy like this { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;aoss:APIAccessAll\u0026#34;, \u0026#34;aoss:DeleteCollection\u0026#34;, \u0026#34;aoss:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;aoss:DashboardsAccessAll\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } After update IAM Role, we will go back to the AWS DynamoDB Console Select Exports and Streams Scroll down and click Create trigger In Create a trigger step Select sync-data-with-dynamodb-stream Click Create trigger Before perform testing, we need to create Environment Variable as the previous step - when we create Replicate Lambda function\nTo testing DynamoDB stream pipeline, we will go to the AWS API Gateway Console\nSelect APIs Select dynamodb-api-gw In dynamodb-api-gw Select POST Select Test At Request body, paste { \u0026#34;name\u0026#34;: \u0026#34;Test DynamoDB Stream Pipeline\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Test\u0026#34; } Scroll down and click Test We can verify the successful creation of data. By the way, Go to the AWS Cloud Watch to view log.\nCloudWatch enables you to monitor your complete stack (applications, infrastructure, network, and services) and use alarms, logs, and events data to take automated actions and reduce mean time to resolution (MTTR). This frees up important resources and allows you to focus on building applications and business value.\nWe can see like this:\nYou can also view in DynamoDB table or OpenSearch Serverless Dashboard, you can also perform delete or modify actions to learn more about DynamoDB operations.\n"
},
{
	"uri": "//localhost:1313/6.terraform/6.2cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": " Execute the command terraform destroy -auto-approve to clean up resources "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-createapigateway/",
	"title": "Create API Gateway",
	"tags": [],
	"description": "",
	"content": "Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \u0026ldquo;front door\u0026rdquo; for applications to access data, business logic, or functionality from your backend services.\nIn this step, we need to create a API Gateway\nThe architecture overview after you complete this step will be as follows\nIn this lap, we will use API Gateway as a Proxy for DynamoDB\nCreate API Gateway Go to the AWS API Gateway Console Select APIs Click Create API In Create API step At REST API, click Build In Create REST API step Select New API At API name, enter dynamodb-api-gw At API endpoint type, choose Regional Click Create API After create success, we will be redirected to dynamodb-api-gw that we just created Create API Gateway Resource to handle data before saving in DynamoDB Select Resources Click Create resource In Create resource step At Resource name, enter lab Click Create resource After creating Resource, we need to create method\nMethod consist of GET, POST, PATCH, PUT, DELETE With the ANY method, it will match with the above methods\nClick lab resource Click Create method In Create method step At Method type, choose POST Select AWS service At AWS Region, choose ap-southeast-1 At AWS service, choose DynamoDB At HTTP method, choose POST At Action type, select Use action name At Action name, enter PutItem At Execution role, enter arn:aws:iam::\u0026lt;you-aws-account-id\u0026gt;:role/\u0026lt;your-api-gateway-role\u0026gt; Click Create method to complete To allow API Gateway to directly put an item into DynamoDB without invoking a Lambda function, we can utilize the Velocity Template Language (VTL) within API Gateway. VTL is a powerful template language used in Amazon API Gateway to map and transform API requests and responses between different data formats, such as between JSON and XML, or to process data before it\u0026rsquo;s returned to end users.\nWe can use VTL to write a put Item command to DynamoDB.\nSelect lab resource Select POST method Select Integration request and click Edit In Integration request Scroll down to Mapping templates Enter application/json Paste #set($timestamp = $context.requestTimeEpoch) { \u0026#34;TableName\u0026#34;: \u0026#34;lab-table\u0026#34;, \u0026#34;Item\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;$context.requestId\u0026#34; }, \u0026#34;type\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;$input.path(\u0026#39;type\u0026#39;)\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;$input.path(\u0026#39;name\u0026#39;)\u0026#34; }, \u0026#34;createdAt\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;$context.requestTimeEpoch\u0026#34; } } } Click Save After creating Resource, you must deploy it to make it callable.\nTo deploy, we choose POST and click Deploy API\nIn Deploy API Select New Stage At Stage name, enter v1 Click Deploy Now, we can test creating DynamoDB data from API Gateway.\nClick Test Enter { \u0026#34;name\u0026#34;: \u0026#34;Test name\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Test type\u0026#34; } Scroll down and click Test After a successful test, we will see results like this.\nYou can repeat this test to create a little bit of data in DynamoDB.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/",
	"title": "Prerequisite",
	"tags": [],
	"description": "",
	"content": "Overview In this step, we will create IAM Role Create a DynamoDB table Create a API Gateway as a Proxy for DynamoDB table Create a S3 Bucket to store DynamoDB export file Create a OpenSearch Serverless Collection Content Create Dynamodb table Create API Gateway Create OpenSearch Serverless Collection Create S3 bucket Prepare IAM Role "
},
{
	"uri": "//localhost:1313/3-replicate/3.2-handledataandsaveinaoss/",
	"title": "Replicate To OpenSearch Serverless",
	"tags": [],
	"description": "",
	"content": "To stream data from S3 Bucket to OpenSearch Serverless, we need to create a Lambda function to index data into OpenSearch Serverless.\nLambda is a serverless compute service, executes your code in response to events, handling compute resources for you.\nCreate Lambda function Go to the AWS Lambda Console Select Functions Click Create function Select Author from scratch Enter replicate-data-handler At Runtime, select Python 3.12 (newest version) Click Change default execution role Select Use an existing role Choose lambda-lab-role Click Create function Download Lambda function In Lambda function Click Upload from, .zip file and select zip file you\u0026rsquo;re installed After uploading the Lambda code, you\u0026rsquo;ll need to configure some environment variables for the code work.\nSelect Configuration Select Environment Click Edit We need to add these Environment variables.\nWith OPENSEARCH_HOST, we can access OpenSearch Serverless Collection that you have just created to get it.\nBefore we perform replicate data, we need to edit Data access policies of Collection to allow Lambda IAM Role have permissions in OpenSearch Serverless Collection.\nData access policies define how your users access the data within your Collections. Data access policies help you manage Collections at scale by automatically assigning access permissions to Collections and indexes that match a specific pattern. Multiple policies can apply to a single resource. It consist of a set of rules, each with three components: a resource type, granted resources, and a set of permissions.\nTo update Data access policies\nGo to the AWS OpenSearch Serverless Console Select Data access policies Click easy-lab-collection In Edit access policy step Click Add principals Add Lambda role arn:aws:iam::\u0026lt;your-aws-account-id\u0026gt;:role/\u0026lt;lambda-role-name\u0026gt; Click Save To begin replicating data\nGo to the AWS Lambda Console\nSelect Functions and click replicate-data-handler funciton\nClick Test\nIn Configure test event step\nSelect Create new event\nEnter replicate-event\nPaste\n{ \u0026#34;Records\u0026#34;: [ { \u0026#34;s3\u0026#34;: { \u0026#34;bucket\u0026#34;: { \u0026#34;name\u0026#34;: \u0026lt;s3-bucket-name\u0026gt; }, \u0026#34;object\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;AWSDynamoDB/\u0026lt;export-folder\u0026gt;/data/\u0026lt;gzip-file\u0026gt;\u0026#34; } } } ] } Click Save và click Test If you encounter a timeout error, it might be because initially attempted to index document without create a predefined index mapping. In this case, you can rerun the Test after creating the index mapping to ensure that subsequent operations proceed smoothly. Additionally, you can increase the timeout setting for the Lambda function to provide more time for the operations to complete successfully.\n"
},
{
	"uri": "//localhost:1313/4-synchronize/4.2-kinesisstream/",
	"title": "Synchronize Data with Kinesis Stream",
	"tags": [],
	"description": "",
	"content": "Amazon Kinesis Data Streams is a serverless streaming data service that simplifies the capture, processing, and storage of data streams at any scale.\nData saved in a Kinesis stream is typically stored as binary data, we need to convert it before consume\nTo using Kinesis Stream to synchronize data from DynamoDB to OpenSearch Service, we need to enable Kinesis Stream in DynamoDB table and create a Lambda function to handle data.\nThe architecture overview after you complete this step will be as follows:\nCreate Kinesis Stream Go to the AWS Kinesis Console Select Data streams Click Create data stream In Create data stream step Enter kinesis-stream-data-lab At Capacity mode, select On-demand Scroll down and click Create data stream Enable Kinesis Stream Go to the AWS DynamoDB Console Select Tables Click lab-table In lab-table Select Exports and Streams At Amazon Kinesis data stream details, click Turn on In Enable Kinesis Stream step Select kinesis-stream-data-lab Select Microsecond Click Turn on stream Create Lambda function You need to download this file, and follow the steps as done in section 4.1 to create the Lambda function with name sync-data-with-kinesis-stream.\nYou also need to edit the IAM Role to add a policy that allows the Lambda function to consume data from Kinesis Streams.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kinesis:GetRecords\u0026#34;, \u0026#34;kinesis:GetShardIterator\u0026#34;, \u0026#34;kinesis:DescribeStream\u0026#34;, \u0026#34;kinesis:DescribeStreamSummary\u0026#34;, \u0026#34;kinesis:ListShards\u0026#34;, \u0026#34;kinesis:ListStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:kinesis:\u0026lt;aws-region\u0026gt;:\u0026lt;aws-account-id\u0026gt;:stream/\u0026lt;kinesis-stream-name\u0026gt;\u0026#34; } ] } After creating a Lambda function, we need to Add trigger for it\nGo to the sync-data-with-kinesis-stream function Click Add trigger In Add trigger step Select Kinesis Select kinesis-stream-data-lab Scroll down and click Add To testing Kinesis Streams pipeline, we will go to the AWS API Gateway Console\nSelect APIs Select dynamodb-api-gw In dynamodb-api-gw Select POST Select Test At Request body, paste { \u0026#34;name\u0026#34;: \u0026#34;Test Kinesis Stream Pipeline\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Test\u0026#34; } Scroll down and click Test We can verify the successful creation of data. By the way, Go to the AWS Cloud Watch to view log.\nCloudWatch enables you to monitor your complete stack (applications, infrastructure, network, and services) and use alarms, logs, and events data to take automated actions and reduce mean time to resolution (MTTR). This frees up important resources and allows you to focus on building applications and business value.\nWe can see like this:\nWe can also view in DynamoDB table or OpenSearch Serverless Dashboard, you can also perform delete or modify actions to learn more about DynamoDB operations.\n"
},
{
	"uri": "//localhost:1313/3-replicate/3.3-checkdata/",
	"title": "Check Data",
	"tags": [],
	"description": "",
	"content": " Check Data Go to the AWS OpenSearch Serverless Console Select Network policies Click Edit In Edit network access policy Click Enable access to OpenSearch Dashboards Click Select collection and choose lab-collection Click Update Now we need select Collections Click Dashboard Click Menu icon Click Dev Tools We can perform the default OpenSearch command to verify whether my index and document have been created successfully or not.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.3-createopensearchserverless/",
	"title": "Create OpenSearch Serverless",
	"tags": [],
	"description": "",
	"content": "OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment.\nIn this step, we need to create IAM Role for Lambda function and API Gateway\nCreate Lambda replicate policy Go to the AWS OpenSearch Console Select Collections Select Create collection In Configure collection settings step Enter lab-collection At Collection type, select Search At Security, choose Easy create Scroll down and click Next In Review and create collection step Click Create collection "
},
{
	"uri": "//localhost:1313/3-replicate/",
	"title": "Replicate Data",
	"tags": [],
	"description": "",
	"content": "Overview Amazon OpenSearch Serverless is a powerful service that enables efficient search queries on data. When integrating it with DynamoDB, there\u0026rsquo;s a need to replicate existing data from DynamoDB to OpenSearch.\nIn this step, we will learn about how to replicate data from Amazon Dynamodb to Amazon OpenSearch Serverless.\nThe architecture overview after you complete this step will be as follows:\nContent Export DynamoDB Data Replicate To OpenSearch Serverless Check Data "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.4-creates3bucket/",
	"title": "Create S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 provides management features so that you can optimize, organize, and configure access to your data to meet your specific business, organizational, and compliance requirements.\nIn this step, we will create a S3 Bucket to save file that exported from DynamoDB\nCreate S3 Bucket Go to the AWS S3 Console Select Bucket Select Create role In Create bucket step Enter dynamodb-export-data-lab Choose ACLs disabled Scroll down and click Create bucket "
},
{
	"uri": "//localhost:1313/4-synchronize/",
	"title": "Synchronize Data",
	"tags": [],
	"description": "",
	"content": "Overview Amazon DynamoDB Streams keeps track of changes made in a DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real time. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified.\nAmazon Kinesis is a suite of services that provide a way to easily collect, process, and analyze real-time data. It is also extremely cost-effective at any scale. Some use cases of Amazon Kinesis include video/audio solutions, website clickstreams, and IoT data. In this lap, we will use it to streams data from DynamoDB to OpenSearch Serverless\nDynamoDB Streams is best suited for capturing granular level changes made to DynamoDB tables and processing stream records using AWS Lambda, while Kinesis Streams is better for producing and consuming large volumes of data and using Kinesis Analytics, Kinesis Firehose, and Lambda to process stream records\nIn this step, we will synchronize Data from DynamoDB to OpenSearch Serverless\nThe overall architecture of this step will look like this:\nContent Synchronize Data With DynamoDB Stream Synchronize Data With Kinesis Stream "
},
{
	"uri": "//localhost:1313/5-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "We\u0026rsquo;ll take the following steps to clean up the resources we created in this lab.\nGo to the AWS OpenSearch Serverless Console Select Collections. Select lab-collection Click Delete and enter confirm Go to the AWS Kinesis Console Select Data streams. Select kinesis-stream-data-lab Click Actions and select Delete, after that enter delete Go to the AWS DynamoDB Console Select Tables Click lab-table Select Actions and click Delete table, after that enter confirm You can do the same for the Lambda function, IAM Role and API Gateway to clearn other resources\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.5-createiamrole/",
	"title": "Prepre IAM Role",
	"tags": [],
	"description": "",
	"content": "In this step, we need to create IAM Role for Lambda function and API Gateway\nCreate Lambda replicate Role Go to the AWS IAM Console Select Roles Select Create role In Select trusted entity step Select AWS service At Service or use case, Select Lambda At Use case, select Lambda Click Create In Add permissions step Search and select AWSLambdaExecute Click Next In Name, review, and create step Enter lambda-lab-role Scroll down and click Create role "
},
{
	"uri": "//localhost:1313/6.terraform/",
	"title": "Implement with Terraform",
	"tags": [],
	"description": "",
	"content": "Overview Terraform is an open source tool that allows you to define Infrastructure as Code - IaC with diverse cloud providers e.g. Alibaba Cloud, AWS, Azure… Terraform helps you manage your system by coding and automating your infrastructure development. You can use Terraform to manage resources such as servers, networks, databases, and others from different cloud providers. In this chapter, we will explore about Terraform and basic work with Terraform Content Initialize resources with Terraform Clean up resources "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]